<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">

		
		<meta name="description" content="IA locales ou distantes, outils, et un chef d'orchestre nommé n8n">
		<meta name="author" content="Antonin Brugnot">

		<meta name="mobile-web-app-capable" content="yes">
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

		<meta name="viewport" content="width=device-width, initial-scale=1.0">

		<title>IA locales ou distantes, outils, et un chef d'orchestre nommé n8n</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/black.css" id="theme">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<!-- Slide d'introduction -->
				<section>
					<h1>IA locales ou distantes, outils, et un chef d'orchestre nommé n8n</h1>
					<p>
						<small>Antonin Brugnot - Onepoint</small>
					</p>
					<aside class="notes">
						Bonjour ! Aujourd'hui on va explorer comment orchestrer des agents IA avec n8n, une plateforme d'automatisation simple et visuelle. On va connecter des modèles IA locaux avec Ollama ou distants, et des outils grâce au protocole MCP. Une approche concrète pour créer des assistants IA vraiment utiles… et maîtrisés.
					</aside>
				</section>

				<!-- Section 1: Présentation de n8n -->
				<section>
					<section>
						<h1>🔧 Présentation de n8n</h1>
					</section>

					<section>
						<h2>Pourquoi n8n ?</h2>
						<ul>
							<li class="fragment">🔗 Connecter vos outils du quotidien</li>
							<li class="fragment">🎨 Interface visuelle intuitive</li>
							<li class="fragment">🚀 No-code/Low-code</li>
							<li class="fragment">🔐 Contrôle de vos données</li>
							<li class="fragment">💰 Open Source</li>
						</ul>
						<aside class="notes">
							n8n permet de connecter facilement tous vos outils : API, bases de données, services cloud. L'interface visuelle rend l'automatisation accessible même aux non-développeurs. Et contrairement à Zapier, vous gardez le contrôle de vos données.
						</aside>
					</section>

					<section>
						<h2>Selfhosted ou Cloud ?</h2>
						<div style="display: flex; justify-content: space-around;">
							<div style="flex: 1;">
								<h3>🏠 Self-hosted</h3>
								<ul>
									<li>Contrôle total</li>
									<li>Données privées</li>
									<li>Personnalisation</li>
									<li>Docker/K8s</li>
								</ul>
							</div>
							<div style="flex: 1;">
								<h3>☁️ Cloud</h3>
								<ul>
									<li>Simplicité</li>
									<li>Maintenance incluse</li>
									<li>Scalabilité</li>
									<li>Support officiel</li>
								</ul>
							</div>
						</div>
						<aside class="notes">
							Deux options : self-hosted pour garder le contrôle total, ou cloud pour la simplicité. Aujourd'hui on se concentre sur le self-hosted avec Docker.
						</aside>
					</section>

					<section>
						<h2>Cas d'usage classiques</h2>
						<ul>
							<li class="fragment">📧 Automatisation email</li>
							<li class="fragment">🔄 Synchronisation de données</li>
							<li class="fragment">📊 Rapports automatiques</li>
							<li class="fragment">🔔 Notifications intelligentes</li>
							<li class="fragment">🛠️ Outils internes</li>
						</ul>
						<aside class="notes">
							Les cas d'usage vont de la simple automatisation d'emails à la création d'outils internes complexes. Avec l'IA, on peut maintenant créer des workflows vraiment intelligents.
						</aside>
					</section>

					<section>
						<h2>Fonctionnalités clés</h2>
						<ul>
							<li class="fragment">⚡ <strong>Triggers</strong> : Webhook, Schedule, Email...</li>
							<li class="fragment">📝 <strong>Variables</strong> : Stockage de données entre étapes</li>
							<li class="fragment">🧩 <strong>Nodes</strong> : +400 intégrations disponibles</li>
							<li class="fragment">🚨 <strong>Gestion d'erreurs</strong> : Retry, fallback...</li>
							<li class="fragment">🔒 <strong>Credentials</strong> : Stockage sécurisé</li>
						</ul>
						<aside class="notes">
							Les triggers déclenchent vos workflows, les variables permettent de passer des données entre étapes, plus de 400 nodes disponibles, et une gestion d'erreurs robuste.
						</aside>
					</section>
				</section>

				<!-- Section 2: Avec les agents IA -->
				<section>
					<section>
						<h1>🤖 Avec les agents IA</h1>
					</section>

					<section>
						<h2>AI Agent Node</h2>
						<ul>
							<li class="fragment">🧠 <strong>Cerveau central</strong> : Prend des décisions</li>
							<li class="fragment">💭 <strong>Prompts système</strong> : Définit le comportement</li>
							<li class="fragment">🔗 <strong>Intégration native</strong> : Connecté aux workflows</li>
							<li class="fragment">📋 <strong>Planning</strong> : Décompose les tâches complexes</li>
						</ul>
						<aside class="notes">
							L'AI Agent Node est le cerveau de votre workflow. Il peut prendre des décisions, planifier des actions et s'adapter selon le contexte.
						</aside>
					</section>

					<section>
						<h2>Ollama local vs distant</h2>
						<div style="display: flex; justify-content: space-around;">
							<div style="flex: 1;">
								<h3>🏠 Ollama Local</h3>
								<ul>
									<li>🔒 Données privées</li>
									<li>⚡ Pas de latence réseau</li>
									<li>💰 Pas de coût API</li>
									<li>🖥️ GPU/CPU local</li>
								</ul>
							</div>
							<div style="flex: 1;">
								<h3>☁️ Modèles distants</h3>
								<ul>
									<li>🚀 Performance optimale</li>
									<li>🔄 Toujours à jour</li>
									<li>📈 Scalabilité</li>
									<li>💳 Pay-per-use</li>
								</ul>
							</div>
						</div>
						<aside class="notes">
							Ollama permet d'utiliser des modèles comme Llama, Qwen, ou Mistral en local. Les modèles distants offrent plus de puissance mais moins de contrôle.
						</aside>
					</section>

					<section>
						<h2>Mémoire</h2>
						<ul>
							<li class="fragment">🧠 <strong>Mémoire de conversation</strong> : Contexte multi-tours</li>
							<li class="fragment">📚 <strong>Vector Store</strong> : Recherche sémantique</li>
							<li class="fragment">💾 <strong>Variables persistantes</strong> : État entre workflows</li>
							<li class="fragment">🔄 <strong>Historique</strong> : Apprentissage des interactions</li>
						</ul>
						<aside class="notes">
							La mémoire permet aux agents de maintenir le contexte, d'apprendre des interactions précédentes et d'accéder à une base de connaissances.
						</aside>
					</section>

					<section>
						<h2>Outils (via MCP)</h2>
						<ul>
							<li class="fragment">🌐 <strong>Web scraping</strong> : Playwright, Puppeteer</li>
							<li class="fragment">🗂️ <strong>Fichiers</strong> : Lecture, écriture, traitement</li>
							<li class="fragment">📊 <strong>Bases de données</strong> : SQL, NoSQL</li>
							<li class="fragment">📧 <strong>Communications</strong> : Email, Slack, Teams</li>
							<li class="fragment">🛠️ <strong>APIs diverses</strong> : CRM, ERP, Cloud...</li>
						</ul>
						<aside class="notes">
							Le protocole MCP (Model Context Protocol) permet aux agents d'utiliser des outils externes : navigateur web, fichiers, bases de données, APIs...
						</aside>
					</section>

					<section>
						<h2>Architecture complète</h2>
						<img src="img/architecture.png" alt="Architecture n8n + IA" style="max-width: 80%;">
						<aside class="notes">
							Voici l'architecture complète : l'utilisateur déclenche n8n, qui utilise un agent IA connecté à des modèles locaux ou distants, avec mémoire et outils.
						</aside>
					</section>
				</section>

				<!-- Section 3: Démo live -->
				<section>
					<section>
						<h1>🚀 Démo live : n8n sandbox</h1>
					</section>

					<section>
						<h2>Les sources : docker-compose.yml</h2>
						<pre><code class="hljs yaml" data-trim>
						postgres:
							image: postgres:15
							...

						n8n:
							build: ./n8n
							...
							ports:
							- 5678:5678

						ollama:
							image: ollama/ollama:latest
							...
							ports:
							- "11435:11434"
						
						# services:
						#   n8n:
						#     models:
						#       - llm
						# models:
						#   llm:
						#     model: ai/model
						</code></pre>
						<aside class="notes">
							Voici la configuration Docker : PostgreSQL pour la persistence, n8n pour l'orchestration, Ollama pour l'IA locale, et Qdrant pour les embeddings. Note : Docker Model Runner avec sa syntaxe `models:` est disponible dans Docker Compose mais pas encore supporté par Podman Compose.
						</aside>
					</section>

					<section>
						<h2>Les workflows</h2>
						<ul>
							<li class="fragment">📧 <strong>Mail.json</strong> : Assistant email intelligent</li>
							<li class="fragment">🗂️ <strong>Indexation.json</strong> : Traitement de documents</li>
							<li class="fragment">🔍 <strong>Search in Index.json</strong> : Recherche sémantique</li>
							<li class="fragment">💬 <strong>Customer Support Chat.json</strong> : Chatbot support</li>
						</ul>
						<aside class="notes">
							On va voir 4 workflows : traitement d'emails avec IA, indexation de documents, recherche sémantique et chatbot de support client.
						</aside>
					</section>

					<section>
						<h2>Demo time! 🎬</h2>
						<h3>Démarrage du stack</h3>
						<pre><code class="hljs bash" data-trim>
						# Lancement des services
						./start.sh

						# Configuration d'Ollama local
						./setup-ollama.sh

						# Import des workflows
						./import-n8n-data.sh
						</code></pre>
						<aside class="notes">
							Maintenant, place à la démo ! On va démarrer le stack, configurer Ollama avec des modèles locaux, et importer nos workflows.
						</aside>
					</section>

					<section data-background="lightblue">
						<h1>🎯 Démo en direct</h1>
						<p>http://localhost:5678</p>
						<aside class="notes">
							[Ici, faire la démonstration live des workflows]
						</aside>
					</section>
				</section>

				<!-- Section 4: Conclusion et questions -->
				<section>
					<section>
						<h1>❓ Conclusion et questions</h1>
					</section>

					<section>
						<h2>Ce qu'on a vu</h2>
						<ul>
							<li class="fragment">✅ n8n comme chef d'orchestre</li>
							<li class="fragment">✅ Intégration IA locale (Ollama) et distante</li>
							<li class="fragment">✅ Agents avec mémoire et outils</li>
							<li class="fragment">✅ Protocole MCP pour étendre les capacités</li>
							<li class="fragment">✅ Démo concrète avec workflows</li>
						</ul>
						<aside class="notes">
							Récap de ce qu'on a couvert : n8n comme plateforme centrale, IA locale et distante, agents intelligents avec mémoire et outils, et une démo pratique.
						</aside>
					</section>

					<section>
						<h2>Pour aller plus loin</h2>
						<ul>
							<li class="fragment">🔗 <strong>Sources</strong> : GitHub avec docker-compose</li>
							<li class="fragment">📚 <strong>Documentation</strong> : n8n.io et ollama.ai</li>
							<li class="fragment">🌐 <strong>Communauté</strong> : Discord n8n et forums</li>
							<li class="fragment">🚀 <strong>Évolutions</strong> : Nouveaux nodes IA en permanence</li>
						</ul>
						<aside class="notes">
							Pour continuer : toutes les sources sont disponibles, la doc officielle est excellente, et la communauté très active.
						</aside>
					</section>

					<section>
						<h2 class="r-fit-text">Questions ? 🤔</h2>
						<p class="fragment">Merci pour votre attention !</p>
					</section>

					<section data-auto-animate>
						<h2 data-id="code-title">bio.yaml</h2>
						<pre data-id="code-animation"><code class="hljs yaml" data-trim data-line-numbers><script type="text/template">
							first_name: "Antonin"
							family_name: "Brugnot"
							company: "Onepoint"
							twitter: null
							personal_info:
							  email: "a.brugnot@groupeonepoint.com"
							  birth: "21st July, 1987"
							  photo: "tronche_joviale.png"
							  location: "Nantes"
							summary: "Lead Tech, DevOps, FullStack, Cloud, IA"
						</script></code></pre>
					</section>

					<section>
						<h2>Contact & Sources</h2>
						<p>📧 a.brugnot@groupeonepoint.com</p>
						<p>🔗 GitHub : antoBrugnot/n8n</p>
						<p>🏢 Onepoint - Nantes</p>
					</section>
				</section>	            
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				controls: true,
				progress: true,
				center: true,
				hash: true,
				slideNumber: true,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [RevealNotes, RevealMarkdown, RevealHighlight ]
			});
		</script>
	</body>
</html>
